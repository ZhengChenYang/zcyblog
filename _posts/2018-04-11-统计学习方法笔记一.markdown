---
layout: post
title:  "统计学习方法笔记一"
date:   2018-04-03 16:20:09 +0800
categories: 机器学习
---

# 统计学习基本概念 #
## 输入空间，特征空间与输出空间 ##
输入的所有可能取值的集合称为 **输入空间** （input space）   
输出的所有可能取值的集合称为 **输出空间** （output space）  
输入空间和输出空间可以是有限的元素集合，亦可以是整个欧式空间。两者可以是同一个空间，也可以是不同的空间；但是通常输出空间远远小于输入空间。  
  
每一个具体的输入是一个实例，通常由 **特征向量** （feature vector）表示。这时，所有特征向量存在的空间称为 **特征空间** （feature space）。

输入变量写作 **X**，输出变量写作 **Y**  
输入实例的 **特征向量** 记作  
	$$ x=(x^{(1)},x^{(2)},...,x^{(n)})^T $$  
**训练集** 通常表示为  
	$$ T=\lbrace(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\rbrace $$  
**回归问题**： 输入变量与输出变量均为连续变量的预测问题。  
**分类问题**： 输出变量为有限个离散变量的预测问题。  
**标注问题**： 输入变量与输出变量均为变量序列的预测问题称为标注问题  

## 假设空间 ##
监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。模型术语由输入空间到输出空间的映射的集合，这个集合就是 **假设空间** （hypothesis space）  

## 统计学习三要素 ##
统计学习方法都是由模型，策略和算法构成的，即统计学习方法 由三要素构成，可以简单的表示为  
$$方法 = 模型 + 策略 + 算法$$   
### 模型 ###
在监督学习过程中，模型就是索要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。   
假设空间用F表示。假设空间可以定义为决策函数的集合  
	$$ F= \lbrace f|Y_θ=f(X), θ∈R^n \rbrace $$  
参数向量θ取值于n维欧式空间\\(R^n\\),称为参数空间（parameter space）  
假设空间也可以定义为条件概率的集合  
	$$ F= \lbrace P|P_θ(Y|X), θ∈R^n \rbrace $$  
参数向量θ取值于n维欧式空间\\(R^n\\),称为参数空间（parameter space） 
### 策略 ###
有了模型的假设空间，统计学习接着需要考虑的是按照怎么样的准则学习或选择最优的模型，统计学习的目标在于从假设空间中选取最优模型。  
首先引入 **损失函数** 与 **风险函数** 的概念。 **损失函数** 度量模型一次预测的好坏， **风险函数** 度量平均意义下模型预测的好坏。  
1. 损失函数（loss function）或代价函数（cost function）    
(1) 0-1损失函数（0-1 loss function）  
	$$
	L(Y,f(X)) =
	\begin{cases}
	0, & \text{Y = f(X)}  \\\\
	1, & \text{Y $\neq$ f(X)}
	\end{cases}
	$$  
(2) 平方损失函数（quadratic loss function）  
	$$ L(Y,f(X)) = (Y-f(X))^2 $$  
(3) 绝对损失函数（absolute loss function）    
	$$ L(Y,f(X)) = |Y-f(X)| $$  
(4) 对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）    
	$$ L(Y,P(X|Y)) = -log{P(Y|X)} $$  
模型f(X)关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作\\(R_{emp}\\):  
	$$ 
	R_{emp}(f) = \frac{1}{N} \sum_{i=1}^N{L(y_i,f(x_i))}
	$$  
期望风险是模型归于联合分布的期望损失，经验风湿是模型关于训练样本集的平均损失。如果样本无限大，经验风险趋于期望风险。但是由于实际样本数量有限，所以需要对经验风险进行一定的矫正。这就用到监督学习的两个基本策略：经验风险最小化和结构风险最小化。  

2. 经验风险最小化与结构风险最小化  
经验风险最小化（empirical risk minimizaiotn）的策略认为，经验风险最小的模型是最优的模型。按照这个经验风险最小化求最优模型就是求解最优化问题：  
	$$ \min_{f∈F} \frac{1}{N} \sum_{i=1}^N{L(y_i,f(x_i))} $$  
结构风险最小化（structural risk minimization）是为了防止过拟合而提出来的策略。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。在假设空间，损失函数以及训练数据集确定的情况下，结构风险的定义是  
	$$ R_{srm}(f) =  \frac{1}{N} \sum_{i=1}^N{L(y_i,f(x_i))} + λJ(f) $$  

### 算法 ###
算法是指学习模型的具体计算方法  

## 模型评估与模型选择 ## 
将学习方法对未知数据的预测能力成为泛化能力（generalization ability）。所选模型的复杂度比真模型高，这种现象成为过拟合（over-fitting）。过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差。  

## 正则化与交叉验证
正则化（regularization）是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化一般具有如下形式：  
	$$ \min_{f∈F} \frac{1}{N} \sum_{i=1}^N{L(y_i,f(x_i))} + λJ(f) $$  
其中第1项是经验风险，第2项是正则化项，λ>=0为调整两者之间关系的整数。正则化项可以取不同的形式。如回归问题中，损失函数是平方损失，正则化项可以使参数向量的\(L_2\)范数:
	$$ L(w)=\frac{1}{N} \sum^{N}_{i=1}(f(x_i;w)-y_i)^2 + \frac{λ}{2}||w||^2 $$  
正则化符合奥卡姆剃刀原理（Occam's razor）。在所有可能选择的模型中，能够很好地解析已知数据并且十分简单才是最好的模型。  

1. 简单交叉验证  
随机地将已给数据分为两部分，一部分作为训练集，一部分作为测试集。然后用训练集在各种条件下（不同参数个数）训练模型，从而得到不同模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。
2. S折交叉验证  
随机地将数据切分为S个互不相交的大小相同的子集。选用S-1个子集的数据训练模型，利用余下大的子集测试模型；将这一过程对可能的S中选择重复进行；最后选出S次测评中评价测试误差最小的模型。

## 泛化能力 ##
学习方法的泛化能力（generalizaiton ability）是指由该方法学习到的模型对未知数据的预测能力。


  



 







  